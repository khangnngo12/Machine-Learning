{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 338,
     "status": "ok",
     "timestamp": 1648752376661,
     "user": {
      "displayName": "Michael Hance",
      "userId": "01498427894019643105"
     },
     "user_tz": 420
    },
    "id": "WKo_jWSLI6PC"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "import torch.nn\n",
    "import torch.optim\n",
    "import torch.utils.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_z6AbT6VTooP"
   },
   "source": [
    "# Problem 1\n",
    "\n",
    "Use `np.histogram` to calculate the [probability density](https://en.wikipedia.org/wiki/Probability_density_function) that values in an arbitrary input data array fall within user-specified bins. Hint: `np.histogram` does all the work for you with the correct arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 221,
     "status": "ok",
     "timestamp": 1648752809896,
     "user": {
      "displayName": "Michael Hance",
      "userId": "01498427894019643105"
     },
     "user_tz": 420
    },
    "id": "txgSi6EUTqXX"
   },
   "outputs": [],
   "source": [
    "def estimate_probability_density(data, bins):\n",
    "    \"\"\"Estimate the probability density of arbitrary data.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data : array\n",
    "        1D numpy array of random values.\n",
    "    bins : array\n",
    "        1D numpy array of N+1 bin edges to use. Must be increasing.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    array\n",
    "        1D numpy array of N probability densities.\n",
    "    \"\"\"\n",
    "    assert np.all(np.diff(bins) > 0) \n",
    "\n",
    "    probabilities, bins_edge = np.histogram(data,bins,density=True) #Find prob dens & bin edge of each bin\n",
    "    return probabilities #Return prob dens store as rho\n",
    "\n",
    "    raise NotImplementedError() #How does this work? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1648752810060,
     "user": {
      "displayName": "Michael Hance",
      "userId": "01498427894019643105"
     },
     "user_tz": 420
    },
    "id": "2xOvjCPZT0hC"
   },
   "outputs": [],
   "source": [
    "# A correct solution should pass these tests.\n",
    "generator = np.random.RandomState(seed=123)\n",
    "data = generator.uniform(size=100)\n",
    "bins = np.linspace(0., 1., 11)\n",
    "rho = estimate_probability_density(data, bins)\n",
    "assert np.allclose(rho, [ 0.6,  0.8,  0.7,  1.7,  1.1,  1.3,  1.6,  0.9,  0.8,  0.5]) #measure\n",
    "data = generator.uniform(size=1000)\n",
    "bins = np.linspace(0., 1., 101)\n",
    "rho = estimate_probability_density(data, bins)\n",
    "dx = bins[1] - bins[0]\n",
    "assert np.allclose(dx * rho.sum(), 1.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a9iMtapqRZcM"
   },
   "source": [
    "# Problem 2\n",
    "\n",
    "Define a function to calculate the [entropy](https://en.wikipedia.org/wiki/Entropy_estimation) $H(\\rho)$ of a binned probability density, defined as:\n",
    "\n",
    "$$H(\\rho) \\equiv -\\sum_i \\rho_i \\log(\\rho_i) \\Delta w_i$$\n",
    "\n",
    "where $\\rho_i$ is the binned density in bin $i$ with width $w_i$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 286,
     "status": "ok",
     "timestamp": 1648752813865,
     "user": {
      "displayName": "Michael Hance",
      "userId": "01498427894019643105"
     },
     "user_tz": 420
    },
    "id": "E1oPDWPcRzUY"
   },
   "outputs": [],
   "source": [
    "def binned_entropy(rho, bins):\n",
    "    \"\"\"Calculate the binned entropy.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    rho : array\n",
    "        1D numpy array of densities, e.g., calculated by the previous function.\n",
    "    bins : array\n",
    "        1D numpy array of N+1 bin edges to use. Must be increasing.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        Value of the binned entropy.\n",
    "    \"\"\"\n",
    "    assert np.all(np.diff(bins) > 0)\n",
    "    \n",
    "    width_i = np.diff(bins) #Array w/ width of each bin\n",
    "    H_entropy = -(np.sum(rho*np.log(rho)*width_i)) #Calculate entropy\n",
    "    \n",
    "    return H_entropy #Return calculated entropy value\n",
    "\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 178,
     "status": "ok",
     "timestamp": 1648752814222,
     "user": {
      "displayName": "Michael Hance",
      "userId": "01498427894019643105"
     },
     "user_tz": 420
    },
    "id": "eHDTdUe_R5j_"
   },
   "outputs": [],
   "source": [
    "# A correct solution should pass these tests.\n",
    "generator = np.random.RandomState(seed=123)\n",
    "data1 = generator.uniform(size=10000)\n",
    "data2 = generator.uniform(size=10000) ** 4\n",
    "bins = np.linspace(0., 1., 11)\n",
    "rho1 = estimate_probability_density(data1, bins)\n",
    "rho2 = estimate_probability_density(data2, bins)\n",
    "H1 = binned_entropy(rho1, bins)\n",
    "H2 = binned_entropy(rho2, bins)\n",
    "assert np.allclose(H1, -0.000801544)\n",
    "assert np.allclose(H2, -0.699349908)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3UH9jbZTR2wN"
   },
   "source": [
    "# Problem 3\n",
    "\n",
    "We're going to implement a network that will do multi-category classification.  We'll base this on the tools we developed in the [NeuralNetworks2](https://git.ucsc.edu/mhance/phys152/-/blob/master/Notebooks/NeuralNetworks2.ipynb) notebook from class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "executionInfo": {
     "elapsed": 6686,
     "status": "ok",
     "timestamp": 1648852688955,
     "user": {
      "displayName": "Michael Hance",
      "userId": "01498427894019643105"
     },
     "user_tz": 420
    },
    "id": "kx_jGSlTEwSF"
   },
   "source": [
    "First let's load in the data file.  You can find the data file in the HW area of my gitlab repo: [HW2_data.h5](https://git.ucsc.edu/mhance/phys152/-/blob/master/HW/HW2_data.h5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "executionInfo": {
     "elapsed": 163,
     "status": "ok",
     "timestamp": 1648852689102,
     "user": {
      "displayName": "Michael Hance",
      "userId": "01498427894019643105"
     },
     "user_tz": 420
    },
    "id": "grgFGAURGLGP"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1279a0e10>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import h5py\n",
    "h5f = h5py.File('HW2_data.h5','r')\n",
    "x_train= h5f['x_train'][:]\n",
    "y_train= h5f['y_train'][:]\n",
    "x_test= h5f['x_test'][:]\n",
    "y_test= h5f['y_test'][:]\n",
    "h5f.close()\n",
    "\n",
    "torch.manual_seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = torch.Tensor(x_train) #ndarray to tensor\n",
    "y_train = torch.Tensor(y_train) #ndarray to tensor\n",
    "x_test = torch.Tensor(x_test) #ndarray to tensor\n",
    "y_test = torch.Tensor(y_test) #ndarray to tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function Tensor.type>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = torch.nn.Sequential(\n",
    "    torch.nn.Linear(4, 20),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(20, 25),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(25, 3),\n",
    ") \n",
    "\n",
    "torch.save(net.state_dict(), 'net_HW2.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.CrossEntropyLoss() #Define Loss Function\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.1) #Define Adam Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "xy_train = torch.utils.data.TensorDataset(x_train, y_train)\n",
    "#loader = torch.utils.data.DataLoader(xy_train, batch_size=5, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'net' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-3c8d13dc8953>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'net_HW2.pth'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxy_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m#stores loss values for training data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mlosses_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m#stores loss values for test data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'net' is not defined"
     ]
    }
   ],
   "source": [
    "net.load_state_dict(torch.load('net_HW2.pth'))\n",
    "loader = torch.utils.data.DataLoader(xy_train, batch_size=200, shuffle=True) \n",
    "losses = [] #stores loss values for training data\n",
    "losses_t = [] #stores loss values for test data\n",
    "\n",
    "net.train() #train in batches of 5\n",
    "for epoch in range(1000):\n",
    "    #net.train()\n",
    "    for x_batch, y_batch in loader:\n",
    "        y_batch = y_batch.type(torch.LongTensor)\n",
    "        y_pred = net(x_batch)\n",
    "        loss = loss_fn(y_pred, y_batch)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    losses.append(loss.data)\n",
    "    \n",
    "    net.eval() #evaluate using current weight\n",
    "    y_test = y_test.type(torch.LongTensor)\n",
    "    y_pred = net(x_test)\n",
    "    losses_t.append(loss_fn(y_pred, y_test).data)\n",
    "    \n",
    "plt.plot(losses, '.', label=\"TRAIN (B=200,L.R.=0.1)\")\n",
    "plt.plot(losses_t, '.',label=\"TEST\")\n",
    "plt.legend()\n",
    "plt.xlabel('Training Epoch')\n",
    "plt.ylabel('CrossEntropyLoss')\n",
    "plt.title(\"Loss vs Epoch\")\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix \n",
      "X-Axis = True Value \n",
      "Y-Axis = Predicted Value \n",
      "Accuracy = 0.78 \n",
      "Error Rate = 0.22\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>32.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>22.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      0     1     2\n",
       "0  32.0   0.0   4.0\n",
       "1   4.0  24.0   7.0\n",
       "2   2.0   5.0  22.0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "prediction = net(torch.Tensor(x_test)) #Use trained network to calculate outputs\n",
    "pairs = list((zip(torch.argmax(prediction,dim=1).numpy(),y_test))) \n",
    "#argmax find index of the largest number in each tensor. Since we have class 0,1,2, index=class\n",
    "#Create pairs. Format: Pred Data,True Data\n",
    "\n",
    "table = np.zeros((3,3)) #Create a 3x3 table of zero\n",
    "for pair in pairs: #Add one to their respective element when their respective pair is identify\n",
    "    if pair == tuple([0,0]):\n",
    "        table[0,0] += 1\n",
    "    elif pair == tuple([1,0]):\n",
    "        table[1,0] += 1\n",
    "    elif pair == tuple([2,0]):\n",
    "        table[2,0] += 1\n",
    "    elif pair == tuple([0,1]):\n",
    "        table[1,0] += 1\n",
    "    elif pair == tuple([1,1]):\n",
    "        table[1,1] += 1\n",
    "    elif pair == tuple([2,1]):\n",
    "        table[2,1] += 1\n",
    "    elif pair == tuple([0,2]):\n",
    "        table[0,2] += 1\n",
    "    elif pair == tuple([1,2]):\n",
    "        table[1,2] += 1\n",
    "    elif pair == tuple([2,2]):\n",
    "        table[2,2] += 1\n",
    "\n",
    "df = pd.DataFrame(table,columns=[0,1,2],index=[0,1,2]) #Create DataFrame for Confusion Matrx\n",
    "Acc = (df[0][0]+df[1][1]+df[2][2])/(df.to_numpy().sum())\n",
    "Err_Rate = (df[0][1]+df[0][2]+df[1][0]+df[1][2]+df[2][0]+df[2][1])/(df.to_numpy().sum())\n",
    "print(\"Confusion Matrix \\nX-Axis = True Value \\nY-Axis = Predicted Value \\nAccuracy = {0} \\nError Rate = {1}\".format(Acc,Err_Rate)) #Print axis label\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9Rp7v2yLGVDv"
   },
   "source": [
    "Now the fun part.  Let's construct a similar network to what we used in class: \n",
    "- 2 hidden layers, with 20 and 25 nodes respectively, using ReLU activation functions.  \n",
    "- The number of input layers should correspond to the number of features in the training/test data, and the number of output layers should correspond to the number of categories in the training/test data.  In this case, the `y_train` and `y_test` lists will show the category label as a number between `0` and `C`, where `C` is the total number of categories.  You should inspect the training/test data to figure out the number of features and the number of categories.\n",
    "- For the loss function, we should use `torch.nn.CrossEntropyLoss()`.  This implements the `softmax` activation function on the output layer that we'll use for multi-category classification.\n",
    "- Let's use the Adam optimizer, starting with a learning rate of 0.1\n",
    "- Break the training data into 5 batches and run for 1000 epochs\n",
    "\n",
    "Then you can train the network!  Keep track of the loss values for the training and test data, and plot them vs the epoch number after the training is completed.\n",
    "\n",
    "The predictions of the network will be a set of `C` numbers for each event, corresponding to the probability that the event is classified as each of the `C` different categories.  We should take the maximum value for each set of `C` numbers to determine the category label.  So if the output for event 0 looks like `[0.8, 0.6, 0.95, 0.10]` and the categories are numbered `0..3` then the predicted label would be `2`.\n",
    "\n",
    "Once the network is trained, let's inspect the output.  Using the predictions for the training data, construct a \"confusion matrix\", showing the true labels on one axis and the predicted labels on the other.\n",
    "\n",
    "I can offer more clarifications on this as the HW period goes on, so feel free to ask questions as you start on this one!"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyP3ns07cB9JHhUEFAuxihak",
   "name": "HW2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
