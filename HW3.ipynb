{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import torch.nn\n",
    "import torch.nn.functional\n",
    "import torch.optim\n",
    "import torch.utils.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1\n",
    "\n",
    "Let's explore hyperparameter optimization.  We'll do something relatively simple that's actually not far from what most people do to study the effect of hyperparameters on their model.  Let's revisit the neural network we implemented to learn the $\\sin$ function in our `NeuralNetworks2` notebook.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "x_train = 6 * torch.rand((1000, 1)) - 3\n",
    "y_train = torch.sin(x_train)\n",
    "x_test = 6 * torch.rand((100, 1)) - 3\n",
    "y_test = torch.sin(x_test)\n",
    "plt.plot(x_test.numpy(), y_test.numpy(), '.', label='TEST')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wrap the tensors into Dataset objects to simplify passing them around:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xy_train = torch.utils.data.TensorDataset(x_train, y_train)\n",
    "xy_test = torch.utils.data.TensorDataset(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create (and save) the baseline model to use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "baseline = torch.nn.Sequential(\n",
    "    torch.nn.Linear(1, 20),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(20, 25),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(25, 1),\n",
    ")\n",
    "torch.save(baseline.state_dict(), 'baseline.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, your job: implement the function below to build a train and test loop using hyperparameters specified as function arguments. Your code should produce a single scatter plot showing the TRAIN and TEST loss curves, similar to the ones we produced earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn(train_data, test_data, model, loss_fn=torch.nn.MSELoss(), n_epochs=200,\n",
    "          batch_size=200, learning_rate=0.1, momentum=0.9, make_plot=True):\n",
    "    \"\"\"Perform a train and test loop with specified hyperparameters.\n",
    "    \n",
    "    Uses SGD optimization and produces a scatter plot of TRAIN and TEST\n",
    "    loss versus epoch on a log-linear scale.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    train_data : torch.utils.data.Dataset\n",
    "        Container for the training input and target tensors to use.\n",
    "    test_data : torch.utils.data.Dataset\n",
    "        Container for the test input and target tensors to use.\n",
    "    model : torch.nn.Module\n",
    "        Neural network model of the data whose parameters will be learned.\n",
    "    loss_fn : callable\n",
    "        Function of (y_out, y_tgt) that calculates the scalar loss to use.\n",
    "        Must support backwards() method.\n",
    "    n_epochs : int\n",
    "        Number of epochs of training to perform.\n",
    "    batch_size : int\n",
    "        Size of each (randomly shuffled) minibatch to use.\n",
    "    learning_rate : float\n",
    "        Learning rate to use for the SGD optimizer.\n",
    "    momentum : float\n",
    "        Momentum to use for the SGD optimizer.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    tuple\n",
    "        Tuple (train, test) of arrays of loss values after each epoch for\n",
    "        the TRAIN and TEST samples, respectively. Both lists should have\n",
    "        length equal to n_epochs.    \n",
    "    \"\"\"\n",
    "    \n",
    "    losses_train, losses_test = [], []\n",
    "    loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "    x_test, y_test = test_data.tensors\n",
    "\n",
    "    # YOUR CODE HERE...  replace the next line with your solution\n",
    "    \n",
    "    optimizer = torch.optim.SGD(model.parameters(),lr=learning_rate,momentum=momentum) #Create SDG optimizer\n",
    "    \n",
    "    for epoch in range(n_epochs): #Start training\n",
    "        model.train()\n",
    "        for x_batch, y_batch in loader:\n",
    "            y_pred = model(x_batch)\n",
    "            loss = loss_fn(y_pred,y_batch)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        losses_train.append(loss.data)\n",
    "        \n",
    "        model.eval() #Evaluate test data using trained network\n",
    "        y_pred = model(x_test)\n",
    "        loss = loss_fn(y_pred,y_test)\n",
    "        losses_test.append(loss.data)\n",
    "        \n",
    "    if make_plot:\n",
    "        plt.plot(losses_train, '.', label='TRAIN')\n",
    "        plt.plot(losses_test, '.', label='TEST')\n",
    "        plt.legend()\n",
    "        plt.xlabel('Training Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.yscale('log');\n",
    "    return losses_train, losses_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your code with the default hyperparameters using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A correct solution should pass these tests.\n",
    "torch.manual_seed(123)\n",
    "baseline.load_state_dict(torch.load('baseline.pth'))\n",
    "train, test = learn(xy_train, xy_test, baseline)\n",
    "assert train[0] > 0.1 and test[0] > 0.1\n",
    "assert train[60] < 1e-3 and test[60] < 1e-3\n",
    "assert train[-1] < 1e-4 and test[-1] < 1e-4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below to establish the same initial state (seed and model parameters) and repeat this learning loop with the optimizer momentum set to zero, to show its effect on the loss curves: large synchronized oscillations in both the TRAIN and TEST data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "baseline.load_state_dict(torch.load('baseline.pth'))\n",
    "learn(xy_train, xy_test, baseline, momentum=0);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll scan different values of the batch size, learning rate, and momentum to find good configurations.  In general it's best to do this by sampling random values of each parameter on any given trial, but for ease of interpretation we'll do it in 1-dimension for a few different sets of configurations.\n",
    "\n",
    "Make plots showing the loss curves for the training data for each of the following sets of configurations:\n",
    "* Learning rate = 0.06, batch size=51, for momenta between 0.79 and 1.0 in steps of 0.05\n",
    "* Momentum=0.9, batch size=51, for learning rates between 0.01 and 0.22 in steps of 0.05\n",
    "* Learning rate = 0.1, momentum=0.9, between 1 and the size of the training set, in steps of 100\n",
    "\n",
    "Some notes:\n",
    "* Make sure to reset the random seed and reload the network before each call to the `learn` function!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking the momentum\n",
    "momentum = np.arange(0.79,1.0,0.05)\n",
    "for momenta in momentum: \n",
    "    #Run using momenta value\n",
    "    torch.manual_seed(123)\n",
    "    baseline.load_state_dict(torch.load('baseline.pth'))\n",
    "    train, test = learn(xy_train, xy_test, baseline, batch_size=51, learning_rate=0.06, momentum=momenta, make_plot=False)\n",
    "    \n",
    "    plt.plot(train, '.', label='TRAIN p={}'.format(round(momenta,2)))\n",
    "    plt.plot(test, '.', label='TEST p={}'.format(round(momenta,2)))\n",
    "    plt.title(\"Changing Momentum\")\n",
    "    plt.legend()\n",
    "    plt.xlabel('Training Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.yscale('log')\n",
    "\n",
    "    #Establish same initial state\n",
    "    torch.manual_seed(123)\n",
    "    baseline.load_state_dict(torch.load('baseline.pth'))\n",
    "    learn(xy_train, xy_test, baseline, momentum=0,make_plot=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking the learning rate\n",
    "lr = np.arange(0.01,0.22,0.05)\n",
    "for lr_val in lr:\n",
    "    torch.manual_seed(123)\n",
    "    baseline.load_state_dict(torch.load('baseline.pth'))\n",
    "    train, test = learn(xy_train, xy_test, baseline, batch_size=51, learning_rate=lr_val, momentum=0.9, make_plot=False)\n",
    "    \n",
    "    plt.plot(train, '.', label='TRAIN lr={}'.format(round(lr_val,2)))\n",
    "    plt.plot(test, '.', label='TEST lr={}'.format(round(lr_val,2)))\n",
    "    plt.title(\"Changing Learning Rate\")\n",
    "    plt.legend()\n",
    "    plt.xlabel('Training Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.yscale('log')\n",
    "\n",
    "    #Establish same initial state\n",
    "    torch.manual_seed(123)\n",
    "    baseline.load_state_dict(torch.load('baseline.pth'))\n",
    "    learn(xy_train, xy_test, baseline, momentum=0,make_plot=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking the batch size\n",
    "bs = np.arange(100,1001,100)\n",
    "for bs_val in bs:\n",
    "    torch.manual_seed(123)\n",
    "    baseline.load_state_dict(torch.load('baseline.pth'))\n",
    "    train, test = learn(xy_train, xy_test, baseline, batch_size=int(bs_val), learning_rate=0.1, momentum=0.9, make_plot=False)\n",
    "    \n",
    "    plt.plot(train, '.', label='TRAIN bs={}'.format(round(bs_val,2)))\n",
    "    plt.plot(test, '.', label='TEST bs={}'.format(round(bs_val,2)))\n",
    "    plt.title(\"Changing Batch Size\")\n",
    "    plt.legend()\n",
    "    plt.xlabel('Training Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.yscale('log')\n",
    "\n",
    "    #Establish same initial state\n",
    "    torch.manual_seed(123)\n",
    "    baseline.load_state_dict(torch.load('baseline.pth'))\n",
    "    learn(xy_train, xy_test, baseline, momentum=0,make_plot=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2\n",
    "\n",
    "Let's play around with convolutional kernels and see if we can make some simple filters to apply to real images.  We'll use the `torchvision` package to transform common image formats:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pytorch `Conv2D` function that we used to implement some convolutional layers is a high-level function call, and manages the kernel weights internally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With square kernels and equal stride\n",
    "m = torch.nn.Conv2d(16, 33, 3, stride=2)\n",
    "\n",
    "# non-square kernels and unequal stride and with padding\n",
    "m = torch.nn.Conv2d(16, 33, (3, 5), stride=(2, 1), padding=(4, 2))\n",
    "\n",
    "# non-square kernels and unequal stride and with padding and dilation\n",
    "m = torch.nn.Conv2d(16, 33, (3, 5), stride=(2, 1), padding=(4, 2), dilation=(3, 1))\n",
    "\n",
    "# generate some random input:\n",
    "input = torch.randn(20, 16, 50, 100)\n",
    "\n",
    "# compute the output:\n",
    "output = m(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the lower-level function `torch.nn.functional.conv2d` to manipulate kernels and tensors directly.  \n",
    "\n",
    "Let's do something simple: let's define a 1-layer 5x5 matrix and convolve it with a 3x3 kernel to produce a 3x3 output feature map.  We'll make the kernel super-simple, the equivalent of an identity matrix for convolutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channels = 1\n",
    "h, w = 5, 5\n",
    "x = torch.randn(1, channels, h, w)\n",
    "print(x)\n",
    "weights = torch.tensor([[0., 0., 0.],\n",
    "                        [0., 1., 0.],\n",
    "                        [0., 0., 0.]])\n",
    "weights = weights.view(1, 1, 3, 3)\n",
    "\n",
    "output = torch.nn.functional.conv2d(x, weights)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we add in some padding, then we can make the output tensor the same size as the input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = torch.nn.functional.conv2d(x, weights,padding=1)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course it's possible to do this for more than just one channel, but that gets a bit tricky, so I'll leave that for you to play with if you want!\n",
    "\n",
    "For now let's just try to do something simple with a one-channel image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "urllib.request.urlretrieve(\"http://mhance.scipp.ucsc.edu/banana-slug-misty-morehead.jpg\", \"banana_slug.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.open(\"banana_slug.jpg\")\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can turn this into a numpy array relatively easily:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npimg=np.asarray(img)\n",
    "plt.imshow(npimg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or we can use `torchvision` to turn it into a pytorch tensor, and then check that the tensor gives us back the image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgtensor=torchvision.io.read_image(\"banana_slug.jpg\")\n",
    "plt.imshow(torchvision.transforms.functional.to_pil_image(imgtensor))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make this a grayscale image so we only need to deal with a single channel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "grayscaleimage=torchvision.transforms.Grayscale()(img)\n",
    "grayscaleimage.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that if we try to show this image using `plt.imshow` it will try to color it in for us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(grayscaleimage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So force it into grayscale:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(grayscaleimage,cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can make this into a tensor for processing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grayscaleimagetensor=torchvision.transforms.functional.to_tensor(grayscaleimage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grayscaleimagetensor=grayscaleimagetensor.view(1,1,674,900)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, your job: \n",
    "* implement a 3x3 kernel that shifts every pixel one step to the right\n",
    "* take the difference between the resulting feature map and the original image\n",
    "* display the resulting image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "weights = torch.tensor([[0., 0., 0.], #3x3 kernel that shift every pixel one step to the right\n",
    "                        [1., 0., 0.],\n",
    "                        [0., 0., 0.]])\n",
    "weights = weights.view(1,1,3,3) #.view to reshape tensor?\n",
    "\n",
    "output = torch.nn.functional.conv2d(grayscaleimagetensor,weights,padding=1) #Shift every pixel to the right\n",
    "\n",
    "#diff = output-grayscaleimagetensor #Take the difference between resulting feature map and the original image\n",
    "diff = grayscaleimagetensor-output\n",
    "reduce_dim_diff = np.squeeze(diff) #Reduce dim(1,1,674,900) to dim(674,900)\n",
    "\n",
    "#Print original image tensor\n",
    "print(\"Original Image:\")\n",
    "print(grayscaleimagetensor)\n",
    "\n",
    "#Print resulting feature map tensor\n",
    "print(\"Resulting Feature Map:\")\n",
    "print(output)\n",
    "\n",
    "#Print difference tensor\n",
    "print(\"Difference:\")\n",
    "print(diff)\n",
    "\n",
    "#Display resulting feature map\n",
    "plt.figure()\n",
    "plt.imshow(np.squeeze(output),cmap=\"gray\")\n",
    "plt.title(\"Resulting Feature Map\")\n",
    "\n",
    "#Display difference between resulting feature map and the original image\n",
    "plt.figure()\n",
    "plt.imshow(reduce_dim_diff,cmap='gray')\n",
    "plt.title(\"Difference Between Resulting and Original Map\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3\n",
    "\n",
    "This is the problem where you should explore the input dataset that you'll use for your final project.\n",
    "\n",
    "I'll do this for a mock dataset, but you should go through a similar exercise with your dataset.\n",
    "\n",
    "I'm importing data that's been stored in the HDF5 format, using the [h5py](https://docs.h5py.org/en/stable/) python package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your data are stored in a different format, then certainly some of the details below will have to change, that's OK!  There are lots of ways to read in CSV files and more complicated file formats into python data structures.\n",
    "\n",
    "If you're using ROOT files (common in high-energy physics), there are also lots of ways to get information from those files into python lists.  If you want to convert a ROOT file into HDF5, you can use [this script](https://github.com/scipp-atlas/mario-mapyde/blob/main/scripts/root2hdf5.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signalfile='lowlevelAna_stops.hf5'\n",
    "urllib.request.urlretrieve(\"http://mhance.scipp.ucsc.edu/%s\" % signalfile, signalfile)\n",
    "\n",
    "backgrfile='lowlevelAna_ttbar.hf5'\n",
    "urllib.request.urlretrieve(\"http://mhance.scipp.ucsc.edu/%s\" % backgrfile, backgrfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's inspect the contents of the HDF5 file just to get a sense for what's there:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(signalfile, 'r') as hdf5file:\n",
    "    print(\"Here are the keys in this file\")\n",
    "    print(hdf5file.keys())\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    # let's access the first key by its index:\n",
    "    group=hdf5file[list(hdf5file.keys())[0]]\n",
    "    print(\"What type does this key have?\")\n",
    "    print(type(group))\n",
    "    print(\"It's a group, so we'll need to look inside of the group to find the dataset\")\n",
    "    print(group.keys())\n",
    "    data=group[\"lowleveltree\"]\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    print(\"These are the fields (features) of the dataset:\")\n",
    "    print(data.dtype.names)\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    print(\"Here are the contents of the first event:\")\n",
    "    print(data[0])\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    print(\"This is the number of jets (numjet) in all events: \")\n",
    "    print(data[\"numjet\"])\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know the format, accessing the data is a bit easier...  let's restrict the list of fields (I'll call them 'branches') we read in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "branches=(\"numjet\",\"numlepton\",\"numbtagjet\",\n",
    "          \"met\",\"metphi\",\n",
    "          \"lepton1pT\",\"lepton1eta\",\"lepton1phi\",\n",
    "          \"lepton2pT\",\"lepton2eta\",\"lepton2phi\",\n",
    "          \"jet1pT\", \"jet1eta\", \"jet1phi\",\"jet1b\",\n",
    "          \"jet2pT\", \"jet2eta\", \"jet2phi\",\"jet2b\",\n",
    "          \"jet3pT\", \"jet3eta\", \"jet3phi\",\"jet3b\",\n",
    "          \"jet4pT\", \"jet4eta\", \"jet4phi\",\"jet4b\",\n",
    "          \"jet5pT\", \"jet5eta\", \"jet5phi\",\"jet5b\",\n",
    "          \"jet6pT\", \"jet6eta\", \"jet6phi\",\"jet6b\")\n",
    "\n",
    "with h5py.File(signalfile, 'r') as hdf5file:\n",
    "    data=hdf5file[list(hdf5file.keys())[0]][\"lowleveltree\"]\n",
    "    num_signal_events=len(data[\"numjet\"])\n",
    "    alldata=data[branches]\n",
    "\n",
    "with h5py.File(backgrfile,'r') as hdf5file:\n",
    "    data=hdf5file[list(hdf5file.keys())[0]][\"lowleveltree\"]\n",
    "    num_backgr_events=len(data[\"numjet\"])\n",
    "    alldata = np.concatenate((alldata,data[branches]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We read in the data as fields with a custom format, which is useful for keeping track of what's what, but \n",
    "let's store this as python lists instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Alldata=[[float(i) for i in j] for j in alldata]\n",
    "print(Alldata[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Alldata` variable now contains all of the data for both signal and background events.  Since this will be for a binary classification problem, we want to keep track of which event is which, so we'll construct a `y` list that has 0's and 1's corresponding to whether each event is background (0) or signal (1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.concatenate((np.ones (num_signal_events), \n",
    "                    np.zeros(num_backgr_events)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can split our full dataset into training and test samples.  However, remember that the data aren't randomized...  the first N events of `Alldata` are all signal events, and the rest of the events are background events.  So we need to shuffle the data first, then pull out training and test samples of a specific size.  The `scikit learn` toolkit has a handy function that does this for us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(Alldata, y, test_size=1000, random_state=123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can look at the input features for our training dataset.  First let's make a map of the feature name to the feature data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_before_scaling={}\n",
    "for b in branches:\n",
    "    X_train_before_scaling[b]=[event[branches.index(b)] for event in X_train]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now plot the data for each feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20,30))\n",
    "fig.tight_layout()\n",
    "for b in range(len(branches)):\n",
    "    ax=fig.add_subplot(9,4,1+b if b<8 else 2+b)\n",
    "    plt.subplots_adjust(hspace=0.3,wspace=0.5)\n",
    "    ax.hist(X_train_before_scaling[branches[b]])\n",
    "    ax.set_xlabel(branches[b])\n",
    "    ax.set_ylabel(\"Events/Bin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks good, but the inputs are all over the place!  Some are strictly positive, some have values in the 1's or 10's while others have values in the 1000's.  This broad range of inputs will likely confuse our network and cause some features to have inappropriate influence on the results.  So we should scale the events for each feature to give a distribution that has mean=0 and variance=1.  We can do this for each feature with the following mapping:\n",
    "\n",
    "$$x \\rightarrow z=\\frac{x-\\mu}{\\sigma}$$\n",
    "\n",
    "where $x$ is the original value, $z$ is the 'scaled' value, $\\mu$ is the mean of all values of $x$, and $\\sigma$ is the standard deviation of $x$.  This isn't hard to do by hand, but again `scikit learn` provides a handy way to do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now scale based on the training data:\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "\n",
    "X_train = sc.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make that handy map for the scaled data so we can look at the data by feature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_after_scaling={}\n",
    "for b in branches:\n",
    "    X_train_after_scaling[b]=[event[branches.index(b)] for event in X_train]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print some values just to see what happens to a typical variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train_before_scaling[\"met\"][:5])\n",
    "print(X_train_after_scaling[\"met\"][:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now re-draw the features after they've been scaled to see what they look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20,30))\n",
    "fig.tight_layout()\n",
    "for b in range(len(branches)):\n",
    "    ax=fig.add_subplot(9,4,1+b if b<8 else 2+b)\n",
    "    plt.subplots_adjust(hspace=0.3,wspace=0.5)\n",
    "    ax.hist(X_train_after_scaling[branches[b]])\n",
    "    ax.set_xlabel(branches[b])\n",
    "    ax.set_ylabel(\"Events/Bin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These look a lot better!\n",
    "\n",
    "One last thing: we've scaled our training data, but not our test data.  Fortunately the `sc` object we created above will remember the transformation that we applied to the training data, so we can transform the test data in exactly the same way (note this is just `transform`, not `fit_transform`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = sc.transform (X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now do this for your data.  Try to get as far as you can.  Depending on the type of data you're analyzing, there may not be anywhere near as many plots to make as what I have above -- that's OK!  The important thing will be to identify what your features are, and make sure that the feature data are scaled appropriately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "import pandas as pd\n",
    "# Google Drive link incase Github link doesn't work\n",
    "GDrive_Link = \"https://drive.google.com/file/d/1ZJP3osriK0isJDyBlyqIhbZ1gvT2N4aH/view?usp=sharing\"\n",
    "print(\"Google Drive link incase GitHub link doesn't work:\")\n",
    "print(GDrive_Link)\n",
    "\n",
    "# Import CSV using raw Github link\n",
    "Github_Link = \"https://raw.githubusercontent.com/khangnngo12/Kinematics-of-M33/main/Redshift.csv\"\n",
    "df = pd.read_csv(Github_Link)\n",
    "print(\"CSV file containing velocities (km/s), RA, Dec, and other informations:\")\n",
    "df\n",
    "#raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display Velocity Map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section will display the velocity map with a fitted ellipse and two parabolas. The data points are created using RA and Dec, color-coded using its measured H-alpha velocities. The parabolas will divide the map into three areas: North, South, and Center. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "center_RA = (1 + (33/60) + (51.75/3600))*15 #in deg\n",
    "center_Dec = (30 + (39/60) + (36.630/3600)) # in deg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_func(x,m,b): #y=mx+b function\n",
    "    return (m*x)+b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification_func(RA,Dec):\n",
    "    \n",
    "    \"\"\"\n",
    "    Function to classified each point into North, South, and Center.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    RA: ndarray,req\n",
    "        Containing RA values used in map\n",
    "    Dec: ndarray, req\n",
    "        Containing Dec values used in map\n",
    "        \n",
    "    Returns\n",
    "    ----------\n",
    "    NorthParabola: ndarray containing bools. True == Inside North parabola\n",
    "    SouthParabola: ndarray containing bools. True == Inside South parabola\n",
    "    Center: ndarray containing bools. True == Inside Center region\n",
    "    \"\"\"\n",
    "    \n",
    "    import cmath\n",
    "    theta = np.radians(-32)\n",
    "    \n",
    "    Dec_Val_NP_List = [] #list containing Dec values along North parabola\n",
    "    Dec_Val_SP_List = [] #list containing Dec values along South parabola\n",
    "    \n",
    "    for RA_Val in RA:\n",
    "    \n",
    "        # find a,b,c for quadratic formula\n",
    "        a_NorthPara = -0.5*np.sin(theta)\n",
    "        b_NorthPara = 0.5*np.cos(theta)\n",
    "        c_NorthPara = NorthPara_XCoor - RA_Val\n",
    "        \n",
    "        a_SouthPara = 0.5*np.sin(theta)\n",
    "        b_SouthPara = -0.5*np.cos(theta)\n",
    "        c_SouthPara = SouthPara_XCoor - RA_Val\n",
    "        \n",
    "        # calculate the discriminan\n",
    "        d_NorthPara = (b_NorthPara**2) - (4*a_NorthPara*c_NorthPara) \n",
    "        d_SouthPara = (b_SouthPara**2) - (4*a_SouthPara*c_SouthPara)\n",
    "        \n",
    "        t_NorthPara = (-b_NorthPara + cmath.sqrt(d_NorthPara))/(2*a_NorthPara) # find t for North Parabola (Notice the plus)\n",
    "        t_SouthPara = (-b_SouthPara - cmath.sqrt(d_SouthPara))/(2*a_SouthPara) # find t for South Parabola (Notice the subtract)\n",
    "        \n",
    "        Dec_Val_NorthPara = 0.5*(t_NorthPara*np.sin(theta) + (t_NorthPara**2)*np.cos(theta)) + NorthPara_YCoor #find corresponding Dec value using t\n",
    "        Dec_Val_NP_List.append(Dec_Val_NorthPara)\n",
    "        \n",
    "        Dec_Val_SouthPara = -0.5*(t_SouthPara*np.sin(theta) + (t_SouthPara**2)*np.cos(theta)) + SouthPara_YCoor\n",
    "        Dec_Val_SP_List.append(Dec_Val_SouthPara)\n",
    "        \n",
    "    NorthParabola = np.array(Dec-np.array(Dec_Val_NP_List) > 0)\n",
    "    SouthParabola = np.array(Dec-np.array(Dec_Val_SP_List) < 0)\n",
    "    Center = []\n",
    "    for index in range(len(RA)):\n",
    "        if NorthParabola[index] == False:\n",
    "            if SouthParabola[index] == False:\n",
    "                Center.append(True)\n",
    "            else:\n",
    "                Center.append(False)\n",
    "        else:\n",
    "            Center.append(False)\n",
    "    \n",
    "    return NorthParabola,SouthParabola,np.array(Center)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from math import pi, cos, sin\n",
    "\n",
    "# Get datapoints from CSV files. \n",
    "df_qop2_sorted = df[\"QOP\"] >= 2\n",
    "\n",
    "fig,ax = plt.subplots(1)\n",
    "ax.set_xlabel(\"Right Ascension (deg)\")\n",
    "ax.set_ylabel(\"Declination (deg)\")\n",
    "ax.set_title(\"M33 Velocity Color Map (QOP >= 2)\")\n",
    "ax.set_xlim(23.2,23.8)\n",
    "ax.set_ylim(30.15,30.9)\n",
    "ax.invert_xaxis()\n",
    "ax.set_aspect('equal')\n",
    "cc_map = ax.scatter(df[\"RA\"][df_qop2_sorted]\n",
    "                    ,df[\"DEC\"][df_qop2_sorted]\n",
    "                    ,c=df[\"Velocity (km/s)\"][df_qop2_sorted],s=7,cmap='magma')\n",
    "plt.colorbar(cc_map,label=\"Velocity (km/s)\")\n",
    "fig.set_figwidth(5)\n",
    "fig.set_figheight(5)\n",
    "plt.grid(True,linestyle=\"--\")\n",
    "\n",
    "#------------------------------\n",
    "\n",
    "#Fit Ellipse\n",
    "u= center_RA       #x-position of the center\n",
    "v= center_Dec    #y-position of the center\n",
    "a= 0.52     #radius on the x-axis\n",
    "b= 0.32      #radius on the y-axis\n",
    "t_rot=math.radians(58) #rotation angle\n",
    "\n",
    "t = np.linspace(0, 2*pi, 100)\n",
    "Ell = np.array([a*np.cos(t) , b*np.sin(t)])  \n",
    "     #u,v removed to keep the same center location\n",
    "R_rot = np.array([[cos(t_rot) , -sin(t_rot)],[sin(t_rot) , cos(t_rot)]])  \n",
    "     #2-D rotation matrix\n",
    "\n",
    "Ell_rot = np.zeros((2,Ell.shape[1]))\n",
    "for i in range(Ell.shape[1]):\n",
    "    Ell_rot[:,i] = np.dot(R_rot,Ell[:,i])\n",
    "\n",
    "plt.plot(u+Ell_rot[0,:] , v+Ell_rot[1,:],c='black',linewidth=0.8)    #rotated ellipse\n",
    "\n",
    "#Semi-minor Axis\n",
    "x_SMi_0 = center_RA + (b*cos(np.radians(32))) #x-coor of left Semi-minor axis point \n",
    "y_SMi_0 = center_Dec - (b*sin(np.radians(32))) #y-coor of left Semi-minor axis point\n",
    "x_SMi_1 = center_RA - (b*cos(np.radians(32))) #x-coor of right Semi-minor axis point\n",
    "y_SMi_1 = center_Dec + (b*sin(np.radians(32))) #y-coor of right Semi-minor axis point\n",
    "plt.scatter(x_SMi_0,y_SMi_0,s=15,c=\"black\")\n",
    "plt.scatter(x_SMi_1,y_SMi_1,s=15,c=\"black\")\n",
    "\n",
    "m_SMi = (y_SMi_1-y_SMi_0)/(x_SMi_1-x_SMi_0) #slope of line through Semi-minor axis \n",
    "b_SMi = y_SMi_0 - (m_SMi*x_SMi_0) #y-inter of line through Semi-minor axis\n",
    "x_SMi = np.linspace(x_SMi_1,x_SMi_0,1000) #x values of line through Semi-minor axis\n",
    "y_SMi = linear_func(x_SMi,m_SMi,b_SMi) #y values of line through Semi-minor axis\n",
    "plt.plot(x_SMi,y_SMi,c='black',linewidth=0.8,linestyle=\"--\")\n",
    "\n",
    "#Semi-major Axis\n",
    "x_SouthSMa = center_RA - (a*np.sin(np.radians(32))) #x-coor of North Semi-major axis point\n",
    "y_SouthSMa = center_Dec - (a*np.cos(np.radians(32))) #y-coor of Nouth Semi-major axis point\n",
    "x_NorthSMa = center_RA + (a*np.sin(np.radians(32))) #x-coor of South Semi-major axis point\n",
    "y_NorthSMa = center_Dec + (a*np.cos(np.radians(32))) #y-coor of South Semi-major axis point\n",
    "plt.scatter(x_NorthSMa,y_NorthSMa,c=\"black\",s=15)\n",
    "plt.scatter(x_SouthSMa,y_SouthSMa,c=\"black\",s=15)\n",
    "\n",
    "m_SMa = (y_NorthSMa-y_SouthSMa)/(x_NorthSMa-x_SouthSMa) #slope of line through Semi-major axis\n",
    "b_SMa = y_SouthSMa - (m_SMa*x_SouthSMa) #y-inter of line through Semi-major axis \n",
    "x_SMa = np.linspace(x_NorthSMa,x_SouthSMa,1000) #x values of line through Semi-major axis\n",
    "y_SMa = linear_func(x_SMa,m_SMa,b_SMa) #y values of line through Semi-major axis\n",
    "plt.plot(x_SMa,y_SMa,c='black',linewidth=0.8,linestyle=\"--\")\n",
    "\n",
    "NorthPara_XCoor = x_NorthSMa - ((19*a/20)*np.sin(np.radians(32))) #coordinates to move North parabola to\n",
    "NorthPara_YCoor = linear_func(NorthPara_XCoor,m_SMa,b_SMa)\n",
    "\n",
    "SouthPara_XCoor = x_SouthSMa + ((19*a/20)*np.sin(np.radians(32))) #coordinate to move South parabola to\n",
    "SouthPara_YCoor = linear_func(SouthPara_XCoor,m_SMa,b_SMa)\n",
    "\n",
    "#Fit Two Parabolas\n",
    "theta = np.radians(-32) #degrees to tilt parabolas\n",
    "t = np.linspace(-10,10,1000)\n",
    "#x = (0.5*t)*np.cos(theta) - (t**2)*np.sin(theta) + x_2\n",
    "#y = (0.5*t)*np.sin(theta) + (t**2)*np.cos(theta) + y_2\n",
    "x_NorthPara = 0.5*(t*np.cos(theta) - (t**2)*np.sin(theta)) + NorthPara_XCoor #x values of North parabola\n",
    "y_NorthPara = 0.5*(t*np.sin(theta) + (t**2)*np.cos(theta)) + NorthPara_YCoor #y values of North parabola\n",
    "plt.plot(x_NorthPara,y_NorthPara,c=\"blue\",linewidth=1,linestyle=\"--\")\n",
    "x_SouthPara = -0.5*(t*np.cos(theta) - (t**2)*np.sin(theta)) + SouthPara_XCoor #x values of South parabola\n",
    "y_SouthPara = -0.5*(t*np.sin(theta) + (t**2)*np.cos(theta)) + SouthPara_YCoor #y values of South parabola\n",
    "plt.plot(x_SouthPara,y_SouthPara,c=\"blue\",linewidth=1,linestyle=\"--\")\n",
    "\n",
    "fig.set_figwidth(5)\n",
    "fig.set_figheight(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The codes below will classified each datapoint into one of the three regions. After it is all sorted, is plotted again but this time color-coded based on which region it belong to. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call classification function to sort each point.\n",
    "NorthParabola,SouthParabola,Center = classification_func(df[\"RA\"],df[\"DEC\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from math import pi, cos, sin\n",
    "\n",
    "#Display Data\n",
    "fig,ax = plt.subplots(1)\n",
    "ax.set_xlabel(\"Right Ascension (deg)\")\n",
    "ax.set_ylabel(\"Declination (deg)\")\n",
    "ax.set_title(\"Color Coded Map\")\n",
    "ax.set_xlim(23.2,23.8)\n",
    "ax.set_ylim(30.15,31)\n",
    "#ax.set_xlim(23,24)\n",
    "#ax.set_ylim(30.2,31.2)\n",
    "ax.invert_xaxis()\n",
    "#ax.set_aspect(1/0.861)\n",
    "ax.set_aspect(\"equal\")\n",
    "plt.grid(True,linestyle=\"--\")\n",
    "\n",
    "# Color code each regions\n",
    "ax.scatter(df[\"RA\"][NorthParabola],df[\"DEC\"][NorthParabola],s=7,c=\"red\",label=\"North\")\n",
    "ax.scatter(df[\"RA\"][SouthParabola],df[\"DEC\"][SouthParabola],s=7,c=\"blue\",label=\"South\")\n",
    "ax.scatter(df[\"RA\"][Center],df[\"DEC\"][Center],s=7,c=\"green\",label=\"Center\")\n",
    "plt.legend()\n",
    "\n",
    "#-------------------------------------\n",
    "\n",
    "#Fit Ellipse\n",
    "u= center_RA       #x-position of the center\n",
    "v= center_Dec    #y-position of the center\n",
    "a= 0.52     #radius on the x-axis\n",
    "b= 0.32      #radius on the y-axis\n",
    "t_rot=math.radians(58) #rotation angle\n",
    "\n",
    "t = np.linspace(0, 2*pi, 100)\n",
    "Ell = np.array([a*np.cos(t) , b*np.sin(t)])  \n",
    "     #u,v removed to keep the same center location\n",
    "R_rot = np.array([[cos(t_rot) , -sin(t_rot)],[sin(t_rot) , cos(t_rot)]])  \n",
    "     #2-D rotation matrix\n",
    "\n",
    "Ell_rot = np.zeros((2,Ell.shape[1]))\n",
    "for i in range(Ell.shape[1]):\n",
    "    Ell_rot[:,i] = np.dot(R_rot,Ell[:,i])\n",
    "\n",
    "plt.plot(u+Ell_rot[0,:] , v+Ell_rot[1,:],c='black',linewidth=0.8)    #rotated ellipse\n",
    "\n",
    "#Semi-minor Axis\n",
    "x_SMi_0 = center_RA + (b*cos(np.radians(32))) #x-coor of left Semi-minor axis point \n",
    "y_SMi_0 = center_Dec - (b*sin(np.radians(32))) #y-coor of left Semi-minor axis point\n",
    "x_SMi_1 = center_RA - (b*cos(np.radians(32))) #x-coor of right Semi-minor axis point\n",
    "y_SMi_1 = center_Dec + (b*sin(np.radians(32))) #y-coor of right Semi-minor axis point\n",
    "plt.scatter(x_SMi_0,y_SMi_0,s=15,c=\"black\")\n",
    "plt.scatter(x_SMi_1,y_SMi_1,s=15,c=\"black\")\n",
    "\n",
    "m_SMi = (y_SMi_1-y_SMi_0)/(x_SMi_1-x_SMi_0) #slope of line through Semi-minor axis \n",
    "b_SMi = y_SMi_0 - (m_SMi*x_SMi_0) #y-inter of line through Semi-minor axis\n",
    "x_SMi = np.linspace(x_SMi_1,x_SMi_0,1000) #x values of line through Semi-minor axis\n",
    "y_SMi = linear_func(x_SMi,m_SMi,b_SMi) #y values of line through Semi-minor axis\n",
    "plt.plot(x_SMi,y_SMi,c='black',linewidth=0.8,linestyle=\"--\")\n",
    "\n",
    "#Semi-major Axis\n",
    "x_SouthSMa = center_RA - (a*np.sin(np.radians(32))) #x-coor of North Semi-major axis point\n",
    "y_SouthSMa = center_Dec - (a*np.cos(np.radians(32))) #y-coor of Nouth Semi-major axis point\n",
    "x_NorthSMa = center_RA + (a*np.sin(np.radians(32))) #x-coor of South Semi-major axis point\n",
    "y_NorthSMa = center_Dec + (a*np.cos(np.radians(32))) #y-coor of South Semi-major axis point\n",
    "plt.scatter(x_NorthSMa,y_NorthSMa,c=\"black\",s=15)\n",
    "plt.scatter(x_SouthSMa,y_SouthSMa,c=\"black\",s=15)\n",
    "\n",
    "m_SMa = (y_NorthSMa-y_SouthSMa)/(x_NorthSMa-x_SouthSMa) #slope of line through Semi-major axis\n",
    "b_SMa = y_SouthSMa - (m_SMa*x_SouthSMa) #y-inter of line through Semi-major axis \n",
    "x_SMa = np.linspace(x_NorthSMa,x_SouthSMa,1000) #x values of line through Semi-major axis\n",
    "y_SMa = linear_func(x_SMa,m_SMa,b_SMa) #y values of line through Semi-major axis\n",
    "plt.plot(x_SMa,y_SMa,c='black',linewidth=0.8,linestyle=\"--\")\n",
    "\n",
    "NorthPara_XCoor = x_NorthSMa - ((19*a/20)*np.sin(np.radians(32))) #coordinates to move North parabola to\n",
    "NorthPara_YCoor = linear_func(NorthPara_XCoor,m_SMa,b_SMa)\n",
    "\n",
    "SouthPara_XCoor = x_SouthSMa + ((19*a/20)*np.sin(np.radians(32))) #coordinate to move South parabola to\n",
    "SouthPara_YCoor = linear_func(SouthPara_XCoor,m_SMa,b_SMa)\n",
    "\n",
    "#Fit Two Parabolas\n",
    "theta = np.radians(-32) #degrees to tilt parabolas\n",
    "t = np.linspace(-10,10,1000)\n",
    "#x = (0.5*t)*np.cos(theta) - (t**2)*np.sin(theta) + x_2\n",
    "#y = (0.5*t)*np.sin(theta) + (t**2)*np.cos(theta) + y_2\n",
    "x_NorthPara = 0.5*(t*np.cos(theta) - (t**2)*np.sin(theta)) + NorthPara_XCoor #x values of North parabola\n",
    "y_NorthPara = 0.5*(t*np.sin(theta) + (t**2)*np.cos(theta)) + NorthPara_YCoor #y values of North parabola\n",
    "plt.plot(x_NorthPara,y_NorthPara,c=\"blue\",linewidth=1,linestyle=\"--\")\n",
    "x_SouthPara = -0.5*(t*np.cos(theta) - (t**2)*np.sin(theta)) + SouthPara_XCoor #x values of South parabola\n",
    "y_SouthPara = -0.5*(t*np.sin(theta) + (t**2)*np.cos(theta)) + SouthPara_YCoor #y values of South parabola\n",
    "plt.plot(x_SouthPara,y_SouthPara,c=\"blue\",linewidth=1,linestyle=\"--\")\n",
    "\n",
    "fig.set_figwidth(5)\n",
    "fig.set_figheight(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histograms Using Velocity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(3)\n",
    "ax[0].hist(df[\"Velocity (km/s)\"][NorthParabola])\n",
    "ax[0].set_xlabel(\"Velocity (km/s)\")\n",
    "\n",
    "ax[1].hist(df[\"Velocity (km/s)\"][SouthParabola])\n",
    "ax[1].set_xlabel(\"Velocity (km/s)\")\n",
    "\n",
    "ax[2].hist(df[\"Velocity (km/s)\"][Center])\n",
    "ax[2].set_xlabel(\"Velocity (km/s)\")\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Representing N,S,C as 0,1,2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def y_train():\n",
    "    \n",
    "    \"\"\"\n",
    "    Assign each datapoint a value based on its location\n",
    "    0 = North, 1 = Center, 2 = South\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    y_train: ndarray containing 0,1,2 corresponing to zones.\n",
    "    \"\"\"\n",
    "    \n",
    "    y_train = []\n",
    "    for index in np.arange(len(df[\"Velocity (km/s)\"])):\n",
    "        if index in np.where(NorthParabola==True)[0]:\n",
    "            y_train.append(0)\n",
    "        elif index in np.where(SouthParabola==True)[0]:\n",
    "            y_train.append(2)\n",
    "        else:\n",
    "            y_train.append(1)\n",
    "    \n",
    "    return np.array(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Display TRUE results used to train the program:')\n",
    "print(*y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Possible X-Train, X-Test, Y-Test? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm still unsure about what I can use as the x-train, x-test, and y-test. I'm just trying out different possibilities here. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One possible x-train I'm thinking about is to select two Dec values and classified each datapoint based on those Dec values then shuffle it. Then train it using y-train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "lim_0 = 30.5 #1st limit\n",
    "lim_1 = 30.7 #2nd limit\n",
    "x_train = []\n",
    "for Dec in df[\"DEC\"]:\n",
    "    if Dec > lim_1:\n",
    "        x_train.append(0)\n",
    "    elif Dec < lim_0:\n",
    "        x_train.append(2)\n",
    "    else:\n",
    "        x_train.append(1)\n",
    "random.shuffle(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from math import pi, cos, sin\n",
    "\n",
    "#Display Data\n",
    "fig,ax = plt.subplots(1)\n",
    "ax.set_xlabel(\"Right Ascension (deg)\")\n",
    "ax.set_ylabel(\"Declination (deg)\")\n",
    "ax.set_title(\"Randomized\")\n",
    "ax.set_xlim(23.2,23.8)\n",
    "ax.set_ylim(30.15,31)\n",
    "#ax.set_xlim(23,24)\n",
    "#ax.set_ylim(30.2,31.2)\n",
    "ax.invert_xaxis()\n",
    "#ax.set_aspect(1/0.861)\n",
    "ax.set_aspect(\"equal\")\n",
    "plt.grid(True,linestyle=\"--\")\n",
    "\n",
    "# Color code each regions\n",
    "ax.scatter(df[\"RA\"][np.where(np.array(x_train)==0)[0]],df[\"DEC\"][np.where(np.array(x_train)==0)[0]],s=7,c=\"red\",label=\"North\")\n",
    "ax.scatter(df[\"RA\"][np.where(np.array(x_train)==1)[0]],df[\"DEC\"][np.where(np.array(x_train)==1)[0]],s=7,c=\"blue\",label=\"South\")\n",
    "ax.scatter(df[\"RA\"][np.where(np.array(x_train)==2)[0]],df[\"DEC\"][np.where(np.array(x_train)==2)[0]],s=7,c=\"green\",label=\"Center\")\n",
    "plt.legend()\n",
    "\n",
    "#-------------------------------------\n",
    "\n",
    "#Fit Ellipse\n",
    "u= center_RA       #x-position of the center\n",
    "v= center_Dec    #y-position of the center\n",
    "a= 0.52     #radius on the x-axis\n",
    "b= 0.32      #radius on the y-axis\n",
    "t_rot=math.radians(58) #rotation angle\n",
    "\n",
    "t = np.linspace(0, 2*pi, 100)\n",
    "Ell = np.array([a*np.cos(t) , b*np.sin(t)])  \n",
    "     #u,v removed to keep the same center location\n",
    "R_rot = np.array([[cos(t_rot) , -sin(t_rot)],[sin(t_rot) , cos(t_rot)]])  \n",
    "     #2-D rotation matrix\n",
    "\n",
    "Ell_rot = np.zeros((2,Ell.shape[1]))\n",
    "for i in range(Ell.shape[1]):\n",
    "    Ell_rot[:,i] = np.dot(R_rot,Ell[:,i])\n",
    "\n",
    "plt.plot(u+Ell_rot[0,:] , v+Ell_rot[1,:],c='black',linewidth=0.8)    #rotated ellipse\n",
    "\n",
    "#Semi-minor Axis\n",
    "x_SMi_0 = center_RA + (b*cos(np.radians(32))) #x-coor of left Semi-minor axis point \n",
    "y_SMi_0 = center_Dec - (b*sin(np.radians(32))) #y-coor of left Semi-minor axis point\n",
    "x_SMi_1 = center_RA - (b*cos(np.radians(32))) #x-coor of right Semi-minor axis point\n",
    "y_SMi_1 = center_Dec + (b*sin(np.radians(32))) #y-coor of right Semi-minor axis point\n",
    "plt.scatter(x_SMi_0,y_SMi_0,s=15,c=\"black\")\n",
    "plt.scatter(x_SMi_1,y_SMi_1,s=15,c=\"black\")\n",
    "\n",
    "m_SMi = (y_SMi_1-y_SMi_0)/(x_SMi_1-x_SMi_0) #slope of line through Semi-minor axis \n",
    "b_SMi = y_SMi_0 - (m_SMi*x_SMi_0) #y-inter of line through Semi-minor axis\n",
    "x_SMi = np.linspace(x_SMi_1,x_SMi_0,1000) #x values of line through Semi-minor axis\n",
    "y_SMi = linear_func(x_SMi,m_SMi,b_SMi) #y values of line through Semi-minor axis\n",
    "plt.plot(x_SMi,y_SMi,c='black',linewidth=0.8,linestyle=\"--\")\n",
    "\n",
    "#Semi-major Axis\n",
    "x_SouthSMa = center_RA - (a*np.sin(np.radians(32))) #x-coor of North Semi-major axis point\n",
    "y_SouthSMa = center_Dec - (a*np.cos(np.radians(32))) #y-coor of Nouth Semi-major axis point\n",
    "x_NorthSMa = center_RA + (a*np.sin(np.radians(32))) #x-coor of South Semi-major axis point\n",
    "y_NorthSMa = center_Dec + (a*np.cos(np.radians(32))) #y-coor of South Semi-major axis point\n",
    "plt.scatter(x_NorthSMa,y_NorthSMa,c=\"black\",s=15)\n",
    "plt.scatter(x_SouthSMa,y_SouthSMa,c=\"black\",s=15)\n",
    "\n",
    "m_SMa = (y_NorthSMa-y_SouthSMa)/(x_NorthSMa-x_SouthSMa) #slope of line through Semi-major axis\n",
    "b_SMa = y_SouthSMa - (m_SMa*x_SouthSMa) #y-inter of line through Semi-major axis \n",
    "x_SMa = np.linspace(x_NorthSMa,x_SouthSMa,1000) #x values of line through Semi-major axis\n",
    "y_SMa = linear_func(x_SMa,m_SMa,b_SMa) #y values of line through Semi-major axis\n",
    "plt.plot(x_SMa,y_SMa,c='black',linewidth=0.8,linestyle=\"--\")\n",
    "\n",
    "NorthPara_XCoor = x_NorthSMa - ((19*a/20)*np.sin(np.radians(32))) #coordinates to move North parabola to\n",
    "NorthPara_YCoor = linear_func(NorthPara_XCoor,m_SMa,b_SMa)\n",
    "\n",
    "SouthPara_XCoor = x_SouthSMa + ((19*a/20)*np.sin(np.radians(32))) #coordinate to move South parabola to\n",
    "SouthPara_YCoor = linear_func(SouthPara_XCoor,m_SMa,b_SMa)\n",
    "\n",
    "#Define two limits\n",
    "plt.hlines(30.5,23.2,23.8,linestyle=\"--\",linewidth=1)\n",
    "plt.hlines(30.7,23.2,23.8,linestyle=\"--\",linewidth=1)\n",
    "\n",
    "fig.set_figwidth(5)\n",
    "fig.set_figheight(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Would I just take a subset of x_train and y_train and use them as x_test and y_test? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = x_train[800:1000] #Possible x_test?\n",
    "y_test = y_train[800:1000] #Possible y_test?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
