{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Restricted Boltzmann Machines with `scikit-learn`\n",
    "\n",
    "Let's spend today looking at Restricted Boltzmann Machines (RBM's).  \n",
    "\n",
    "Recall that RBM's learn to associate the ground states of an energy function with the training data.  This is similar to the behavior of generative models like (variational) AutoEncoders, GAN's, and so on that learn latent variables that capture the features of the training data.  In the case of RBM's, the hidden nodes play the role of the latent space, while the visible nodes represent both the input and output layers.  \n",
    "\n",
    "We'll start by implementing our own simple RBM, but then we'll use the `scikit-learn` implementation of RBM's to look at denoising and image reconstruction.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple RBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import scipy \n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from sklearn.neural_network import BernoulliRBM\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from joblib import dump, load\n",
    "\n",
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "matplotlib.rcParams['figure.dpi']=300 # highres display\n",
    "\n",
    "from IPython.display import clear_output\n",
    "from time import sleep\n",
    "\n",
    "from torchvision.datasets import MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions\n",
    "\n",
    "Perform a single step of the Markov chain, going from visible units $v$ to hidden units $h$, according to biases $b$ and weights $w$.\n",
    "    \n",
    "$$z_j = b_j + \\sum_i v_i w_{ij}$$\n",
    "    \n",
    "and \n",
    "    \n",
    "$$P(h_j=1|v) = \\sigma(z_j)$$\n",
    "    \n",
    "Note: you can go from $h$ to $v$, by inserting instead of $v$ the $h$, instead of $b$ the $c$, and instead of $w$ the transpose of $w$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BoltzmannStep(v,b,w,do_random_sampling=True):\n",
    "    batchsize=np.shape(v)[0]\n",
    "    hidden_dim=np.shape(w)[1]\n",
    "    z=b+np.dot(v,w)\n",
    "    \n",
    "    # sigmoid!\n",
    "    P=1/(np.exp(-z)+1)\n",
    "    \n",
    "    # now, the usual trick to obtain 0 or 1 according\n",
    "    # to a given probability distribution:\n",
    "    # just produce uniform (in [0,1]) random numbers and\n",
    "    # check whether they are below the cutoff given by P\n",
    "    if do_random_sampling:\n",
    "        p=np.random.uniform(size=[batchsize,hidden_dim])\n",
    "        return(np.array(p<=P,dtype='int'))\n",
    "    else:\n",
    "        return(P) # no binary random output, just the prob. distribution itself!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function below performs one sequence of steps $v \\to h \\to v' \\to h'$ of a Boltzmann machine, with the given weights $w$ and biases $b$ and $c$.\n",
    "\n",
    "All the arrays have a shape `[batchsize,num_neurons]` (where `num_neurons` is `num_visible` for $v$ and `num_hidden` for $h$).\n",
    "\n",
    "You can set `drop_h_prime` to `True` if you want to use this routine to generate arbitrarily long sequences by calling it repeatedly (then don't use $h'$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BoltzmannSequence(v,b,c,w,\n",
    "                      drop_h_prime=False,\n",
    "                      do_random_sampling=True,\n",
    "                      do_not_sample_h_prime=False,\n",
    "                      do_not_sample_v_prime=False):\n",
    "\n",
    "    # update h based on v\n",
    "    h=BoltzmannStep(v,c,w,do_random_sampling=do_random_sampling)\n",
    "\n",
    "    # sample v based on h, get v'\n",
    "    if do_not_sample_v_prime:\n",
    "        v_prime=BoltzmannStep(h,b,np.transpose(w),do_random_sampling=False)\n",
    "    else:\n",
    "        v_prime=BoltzmannStep(h,b,np.transpose(w),do_random_sampling=do_random_sampling)\n",
    "    \n",
    "    # sample h based on v', get h'\n",
    "    if not drop_h_prime:\n",
    "        if do_not_sample_h_prime: # G. Hinton recommends not sampling in the v'->h' step (reduces noise)\n",
    "            h_prime=BoltzmannStep(v_prime,c,w,do_random_sampling=False)\n",
    "        else:\n",
    "            h_prime=BoltzmannStep(v_prime,c,w,do_random_sampling=do_random_sampling)\n",
    "    else:\n",
    "        h_prime=np.zeros(np.shape(h))\n",
    "        \n",
    "    # return original v along with corresponding h, v', h'\n",
    "    return(v,h,v_prime,h_prime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a set of randomly selected training samples $v$ (of shape `[batchsize,num_neurons_visible]`), and given biases $b,c$ and weights $w$: update those biases and weights according to the contrastive-divergence update rules:\n",
    "\n",
    "$$\\Delta w_{ij} = \\eta \\left( \\langle v_i h_j\\rangle - \\epsilon\\langle v'_i h'_j\\rangle \\right)\\\\\n",
    "\\Delta c_i  = \\eta \\left( \\langle v_i\\rangle - \\epsilon\\langle v'_i\\rangle\\right)\\\\\n",
    "\\Delta b_j  = \\eta \\left( \\langle h_j\\rangle - \\epsilon\\langle h'_j\\rangle\\right)\n",
    "$$\n",
    "\n",
    "Returns `delta_b`, `delta_c`, `delta_w`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainStep(v,b,c,w,\n",
    "              eta,\n",
    "              epsilon=1.,\n",
    "              do_random_sampling=True,\n",
    "              do_not_sample_h_prime=False,\n",
    "              do_not_sample_v_prime=False):\n",
    "    \n",
    "    # update v, h, v', h' based on random sampling\n",
    "    v,h,v_prime,h_prime=BoltzmannSequence(v,b,c,w,\n",
    "                                          do_random_sampling=do_random_sampling,\n",
    "                                          do_not_sample_h_prime=do_not_sample_h_prime,\n",
    "                                          do_not_sample_v_prime=do_not_sample_v_prime)\n",
    "    \n",
    "    # Check difference of random sampling results from training data, update weights and biases accordingly\n",
    "    #       Positive phase                                   Negative phase\n",
    "    db=eta*(np.average(v,axis=0)                       - epsilon*np.average(v_prime,axis=0))\n",
    "    dc=eta*(np.average(h,axis=0)                       - epsilon*np.average(h_prime,axis=0))\n",
    "    dw=eta*(np.average(v[:,:,None]*h[:,None,:],axis=0) - epsilon*np.average(v_prime[:,:,None]*h_prime[:,None,:],axis=0))\n",
    " \n",
    "    return (db, dc, dw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Produce `batchsize` samples, of length `num_visible`. Returns array $v$ of shape `[batchsize,num_visible]`.\n",
    "\n",
    "Within the set of visible nodes, randomly places segments of '1's of size 2*`max_distance`.  So if `max_distance` is the default, then it will place a segment of length 6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def produce_samples_random_segment(batchsize,num_visible,max_distance=3,debug=False):\n",
    "    random_pos=num_visible*np.random.uniform(size=batchsize)\n",
    "    if debug:\n",
    "        print(random_pos)\n",
    "        print(random_pos[:,None])\n",
    "    j=np.arange(0,num_visible)\n",
    "\n",
    "    retval=np.array( np.abs(j[None,:]-random_pos[:,None])<=max_distance, dtype='int' )\n",
    "    \n",
    "    if debug:\n",
    "        print(np.array( np.abs(j[None,:]-random_pos[:,None])))\n",
    "    \n",
    "    return( retval )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's just check to make sure we understand the samples that get produced with this function, using a batch size of 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "produce_samples_random_segment(1,20,max_distance=3,debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RBM Training\n",
    "\n",
    "Now we're finally ready to do the training.  Initialize some hyperparameters and the assign random values as starting points for vectors $b$, $c$, and matrix $w$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_visible=20\n",
    "num_hidden=10\n",
    "eta=0.1\n",
    "epsilon=1\n",
    "nsteps=int(500/epsilon) # increase training sample size with unlearning rate\n",
    "batchsize=50 \n",
    "skipsteps=int(10/epsilon)\n",
    "\n",
    "b=np.random.randn(num_visible)\n",
    "c=np.random.randn(num_hidden)\n",
    "w=np.random.randn(num_visible,num_hidden)\n",
    "\n",
    "test_samples=np.zeros([num_visible,nsteps])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now produce some random assortments of bits that will represent our training data, and train the RBM on those samples as we get them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(nsteps):\n",
    "    \n",
    "    # produce the samples\n",
    "    v=produce_samples_random_segment(batchsize,num_visible)\n",
    "    \n",
    "    # train the RBM on the samples\n",
    "    db,dc,dw=trainStep(v,b,c,w,eta,epsilon=epsilon)\n",
    "    \n",
    "    # update the parameters for the next round of training\n",
    "    b+=db\n",
    "    c+=dc\n",
    "    w+=dw\n",
    "    \n",
    "    # add the visible nodes from the sampling to the test samples...\n",
    "    # not strictly necessary, but we'll visualize the test samples below.\n",
    "    test_samples[:,j]=v[0,:]\n",
    "    \n",
    "    # plot some of the samples below\n",
    "    if j%skipsteps==0 or j==nsteps-1:\n",
    "        clear_output(wait=True)\n",
    "        plt.figure(figsize=(10,1))\n",
    "        plt.imshow(test_samples,origin='lower',aspect='auto',interpolation='none')\n",
    "        plt.axis('off')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now: visualize the typical samples generated (from some starting point).  Run several times to continue this. It basically is a random walk through the space of all possible configurations, hopefully according to the probability distribution that has been learned in training.\n",
    "\n",
    "Note in this case we're explicitly *not* updating $h$, since we're not training anymore!  We just allow the visible nodes $v$ to interact with the latent space $h$ to evolve $v$ as we step through.  This implies that we should get some continuous-ish behavior of the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nsteps=100\n",
    "test_samples=np.zeros([num_visible,nsteps])\n",
    "\n",
    "v_prime=np.zeros(num_visible)\n",
    "h=np.zeros(num_hidden)\n",
    "h_prime=np.zeros(num_hidden)\n",
    "\n",
    "for j in range(nsteps):\n",
    "    v,h,v_prime,h_prime=BoltzmannSequence(v,b,c,w,drop_h_prime=True) # step from v via h to v_prime!\n",
    "    test_samples[:,j]=v[0,:]\n",
    "    v=np.copy(v_prime) # use the new v as a starting point for next step!\n",
    "    if j%skipsteps==0 or j==nsteps-1:\n",
    "        clear_output(wait=True)\n",
    "        plt.imshow(test_samples,origin='lower',interpolation='none')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this is entirely based on *sampling*, not backpropagation, so the algorithm will behave differently overall from the neural nets that we're used to.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RBM's in `scikit-learn`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use the `scikit-learn` implementation of RBM's to look at denoising and image reconstruction.  Let's get some MNIST data to use for training and testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\n",
    "(X_train, X_test), (X_test, Y_test) = mnist.load_data()\n",
    "\n",
    "X_train = X_train.reshape(-1, 784)/255\n",
    "X_test = X_test.reshape(-1, 784)/255\n",
    "\n",
    "# Apply a threshold to binarize the image.\n",
    "X_train = np.where(X_train > 0.2, 1, 0)\n",
    "X_test = np.where(X_test > 0.2, 1, 0)\n",
    "\n",
    "# Split into training and validation sets\n",
    "X_train, X_val = train_test_split(X_train, test_size=1/5,random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do our usual printout of a few figures in MNIST to remind ourselves of what the images are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,4))\n",
    "\n",
    "for i in range(10):\n",
    "  plt.subplot(2,5,i+1)\n",
    "  plt.xticks([])\n",
    "  plt.yticks([])\n",
    "  plt.grid(False)\n",
    "  plt.imshow(X_train[i].reshape(28,28), cmap='Greys')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "I pre-trained a model last night, since it takes a bit of time.  The `scikit-learn` RBM model evaluates a \"pseudo-likelihood\" as the equivalent of a loss function to determine convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # If you have access to our pretrained model\n",
    "    rbm = load('rbm.joblib')  \n",
    "    print(\"RBM Reloaded\")\n",
    "except:\n",
    "    rbm = BernoulliRBM(random_state=0, n_components=80,\n",
    "                       verbose=True, batch_size=20, n_iter=60, learning_rate=0.01)\n",
    "\n",
    "    rbm.fit(X_train)\n",
    "    dump(rbm, 'rbm.joblib')\n",
    "\n",
    "print(\"Training set Pseudo-Likelihood =\", rbm.score_samples(X_train).mean())\n",
    "print(\"Validation set Pseudo-Likelihood =\", rbm.score_samples(X_val).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Denoising\n",
    "\n",
    "Now let's do a de-noising example.  Take an example from the test set and make it noisy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick a random image from the test set\n",
    "im_ind = 23\n",
    "X_pick = X_test[im_ind]\n",
    "\n",
    "# Choose 50 random pixels to flip\n",
    "pick = np.random.choice(28 * 28, 50)\n",
    "x_noisy = np.copy(X_pick)\n",
    "x_noisy[pick] = ((X_pick[pick] + 1) % 2)\n",
    "\n",
    "# Plotting the images\n",
    "fig, ax = plt.subplots(1, 2, figsize=(6, 3))\n",
    "ax[0].imshow(X_pick.reshape(28, 28), cmap='Greys')\n",
    "ax[0].set_title('Original')\n",
    "ax[1].imshow(x_noisy.reshape(28, 28), cmap='Greys')\n",
    "ax[1].set_title('Corrupted')\n",
    "\n",
    "for i in range(2):\n",
    "    ax[i].set_xticks([])\n",
    "    ax[i].set_yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now pass the noisy image into the RBM, which has already been trained, so we're only going to do the Gibbs sampling here to update the visible nodes.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the denoising\n",
    "k_iter = 12  # Number of Gibbs Sampling Iterations\n",
    "alpha = 0.9  # Decay factor for the averaging\n",
    "\n",
    "# Gibb sampling steps.  Do the first iteration based on the noisy image:\n",
    "v = rbm.gibbs(x_noisy)\n",
    "x_final = np.zeros(784) + np.copy(v)\n",
    "fig,ax=plt.subplots(1,k_iter,figsize=(20,10))\n",
    "for i in range(k_iter):\n",
    "    x_slice = np.where(x_final > 0.5*np.max(x_final),1,0)\n",
    "    ax[i].imshow(x_slice.reshape(28,28),cmap='Greys')\n",
    "    ax[i].set_xticks([])\n",
    "    ax[i].set_yticks([])\n",
    "    \n",
    "    # update the image with each iteration.\n",
    "    v = rbm.gibbs(v)\n",
    "    \n",
    "    # Averaging the images\n",
    "    x_final += (alpha**(i+1))*v.astype(float) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying a threshold to binarize the image\n",
    "x_final = np.where(x_final > 0.5*np.max(x_final), 1, 0)\n",
    "\n",
    "plt.imshow(x_final.reshape(28, 28), cmap='Greys')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: play with the noise levels, test images, and more -- how far can we push this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing image reconstruction\n",
    "\n",
    "Finally, let's see if we can reconstruct an image where part of the image is missing entirely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick another random image and set some parts of the image to zero.\n",
    "im_ind = 30\n",
    "X_missing = X_test[im_ind].copy().reshape(28,28)\n",
    "X_missing[:,15:] = 0\n",
    "\n",
    "# Image Reconstruction\n",
    "k_iter = 100 # Number of Gibbs iterations\n",
    "alpha = 0.9  # Decay factor\n",
    "\n",
    "X_recon = np.zeros((28,13)) # Array to store the reconstruction\n",
    "\n",
    "b = X_missing.copy().reshape(-1)\n",
    "for i in range(k_iter):\n",
    "    b = rbm.gibbs(b)\n",
    "    X_recon += alpha**(i) * b.reshape(28,28)[:,15:]\n",
    "    b.reshape(28,28)[:,:15] = X_missing[:,:15]\n",
    "\n",
    "# Apply a threshold and complete the image\n",
    "X_recon = np.where(X_recon > 0.5*np.max(X_recon), 1, 0)\n",
    "X_complete = X_missing.copy()\n",
    "X_complete[:,15:] = X_recon\n",
    "\n",
    "# Plot the figures\n",
    "fig, ax = plt.subplots(1, 3, figsize=(10, 3))\n",
    "ax[0].imshow(X_test[im_ind].reshape(28, 28), cmap='Greys')\n",
    "ax[0].set_title('Original')\n",
    "ax[1].imshow(X_missing, cmap='Greys')\n",
    "ax[1].set_title('Corrupted')\n",
    "ax[2].imshow(X_complete.reshape(28,28), cmap='Greys')\n",
    "ax[2].set_title('Reconstructed')\n",
    "for i in range(3):\n",
    "    ax[i].set_xticks([])\n",
    "    ax[i].set_yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: How much of the image can we chop away?  Does it matter which part of the image we remove?  How many iterations do we need to get back to a good sample?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
